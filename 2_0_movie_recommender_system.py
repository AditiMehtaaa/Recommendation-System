# -*- coding: utf-8 -*-
"""2.0_Movie_Recommender_System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1noPwLtgPZjUKjPdc2iKQTKVz7B9sAk87
"""

pip install scikit-surprise

import pandas as pd
import numpy as np
from surprise import Dataset, Reader, SVD

"""**Data Collection**"""

movie_title = pd.read_csv('Movie_Id_Titles.csv')
movie_ratings = pd.read_csv('Dataset.csv')

movie_title

movie_ratings

"""**Exploratory Data Analysis**"""

# For Movies title
movie_title.head()

movie_title.tail()

movie_title.info()

# For Movies Ratings
movie_ratings.tail()

movie_ratings.head()

movie_ratings.info()

movie_ratings.describe()

"""Combine two Dataset"""

movie_ratings = pd.merge(movie_ratings, movie_title, on = 'item_id')

movie_ratings.head(15)

movie_ratings.describe()

"""**Data Preprocessing**"""

movie_ratings.drop('timestamp', axis=1, inplace=True)

movie_ratings.head()

ndataset = movie_ratings.groupby('title')['rating'].describe()

ndataset

ndataset = ndataset.reset_index()

ndataset.head()

ndataset = ndataset[['title', 'count']]

ndataset.head()

ndataset.sort_values('count', ascending=False)

"""**Modelling**"""

movie_matrix = movie_ratings.pivot_table(index = 'user_id', columns = 'title', values = 'rating', fill_value=0)

movie_matrix.head()

x_star_wars = movie_matrix['Star Wars (1977)']
x_liar_liar = movie_matrix['Liar Liar (1997)']
x_liar_liar.head()

x_star_wars.head()

similar_to_star_wars = pd.DataFrame(movie_matrix.corrwith(x_star_wars), columns=['Correlation value']) #sci-fi movie
similar_to_liar_liar = pd.DataFrame(movie_matrix.corrwith(x_liar_liar), columns=['Correlation value']) #comedy movie

similar_to_star_wars.head()

similar_to_star_wars.dropna(inplace=True)

similar_to_star_wars.head()

similar_to_star_wars = pd.merge(similar_to_star_wars, ndataset, on = 'title')

similar_to_star_wars.head()

similar_to_star_wars = similar_to_star_wars.sort_values('Correlation value', ascending = False)

similar_to_star_wars.head()

sm= similar_to_star_wars[similar_to_star_wars['count']>=100]

movie_ratings = pd.merge(movie_ratings, similar_to_star_wars, on='title')

movie_ratings

"""**Splitting the data**"""

movie_ratings.shape

train_dataset = movie_ratings.iloc[:int(movie_ratings.shape[0]*0.80)]
test_dataset = movie_ratings.iloc[int(movie_ratings.shape[0]*0.80):]

test_dataset.shape

train_dataset.shape

"""**Collaborative Filtering**"""

from surprise import SVD
# from surprise.model_selection import train_test_split
from surprise import accuracy

# create a surprise Dataset
reader = Reader (rating_scale = (1, 5))
train_data = Dataset.load_from_df(train_dataset[['user_id', 'item_id', 'rating']], reader)

# split the data into training and test data sets
trainset = train_data.build_full_trainset()

# create a surprise Dataset
reader = Reader (rating_scale = (1, 5))
test_data = Dataset.load_from_df(test_dataset[['user_id', 'item_id', 'rating']], reader)

# split the data into training and test data sets
testset = test_data.build_full_trainset()

trainset



model = SVD(n_factors=100, biased=True, random_state=15,
          verbose=True)
model.fit(trainset)

# Make predictions on the test set
predictions = model.test(trainset.build_testset())
train_pred = np.array([pred.est for pred in predictions])

# storing test predictions
# getting predictions of testset
test_predictions = model.test(testset.build_testset())
test_pred = np.array([pred.est for pred in test_predictions])



# Creating a sparse matrix
from scipy import sparse
from scipy.sparse import csr_matrix
train_sparse_matrix = sparse.csr_matrix((train_dataset.rating.values,
                                         (train_dataset.user_id.values,
                                          train_dataset.item_id.values)))

# average ratings of all movies given by all users
# average ratings of a particular movie given by all users
# average ratings of all movies given by a particular user

train_averages = dict()
# get the global average of ratings in our trainset.
train_global_average = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()
train_averages['global'] = train_global_average
train_averages

# create a function that takes the sparse matrix as input and gives the average ratings of a movie
# given by all users, and the average rating of all movies given by a single user.

# get the user averages in dictionary (key: user_id/movie_id, value: avg rating)
def get_average_ratings(sparse_matrix, of_users):
  # average ratings of user/axes
  ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes
  # ".A1" is for converting Column Matrix to 1-D numpy array
  sum_of_ratings = sparse_matrix.sum(axis=ax).A1

  # boolean matrix of ratings (whether a user rated that movie or not)
  is_rated = sparse_matrix!=0

  # no of ratings that each user OR movie..
  no_of_ratings = is_rated.sum(axis=ax).A1

  # max_user and max_movie ids in sparse matrix
  u,m = sparse_matrix.shape
  # create a dictionary of users and their average ratings..
  average_ratings = {i :
                     sum_of_ratings[i]/no_of_ratings[i]
                     for i in range(u if of_users else m)
                     if no_of_ratings[i] !=0}

  # return that dictionary of average ratings
  return average_ratings

# the average rating given by a user:
train_averages['user'] = get_average_ratings(train_sparse_matrix, of_users=True)
print('\nAverage rating of user 10:', train_averages['user'][10])

# average ratings given for a movie:
train_averages['movie'] = get_average_ratings(train_sparse_matrix, of_users=False)
print('\nAverage rating of movie 15:', train_averages['movie'][15])

# get users, movies and ratings from our samples train sparse matrix
train_users, train_movies, train_ratings = sparse.find(train_sparse_matrix)

from sklearn.metrics.pairwise import cosine_similarity

from datetime import datetime
final_data = pd.DataFrame()
count = 0
for (user, movie, rating)  in zip(train_users, train_movies, train_ratings):
            st = datetime.now()
        #     print(user, movie)
            #--------------------- Ratings of "movie" by similar users of "user" ---------------------
            # compute the similar Users of the "user"
            user_sim = cosine_similarity(train_sparse_matrix[user], train_sparse_matrix).ravel()
            top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.
            # get the ratings of most similar users for this movie
            top_ratings = train_sparse_matrix[top_sim_users, movie].toarray().ravel()
            # we will make it's length "5" by adding movie averages to .
            top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])
            top_sim_users_ratings.extend([train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))
        #     print(top_sim_users_ratings, end=" ")


            #--------------------- Ratings by "user"  to similar movies of "movie" ---------------------
            # compute the similar movies of the "movie"
            movie_sim = cosine_similarity(train_sparse_matrix[:,movie].T, train_sparse_matrix.T).ravel()
            top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.
            # get the ratings of most similar movie rated by this user..
            top_ratings = train_sparse_matrix[user, top_sim_movies].toarray().ravel()
            # we will make it's length "5" by adding user averages to.
            top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])
            top_sim_movies_ratings.extend([train_averages['user'][user]]*(5-len(top_sim_movies_ratings)))
        #     print(top_sim_movies_ratings, end=" : -- ")

            #-----------------prepare the row to be stored in a file-----------------#
            row = list()
            row.append(user)
            row.append(movie)
            # Now add the other features to this data...
            row.append(train_averages['global']) # first feature
            # next 5 features are similar_users "movie" ratings
            row.extend(top_sim_users_ratings)
            # next 5 features are "user" ratings for similar_movies
            row.extend(top_sim_movies_ratings)
            # Avg_user rating
            row.append(train_averages['user'][user])
            # Avg_movie rating
            row.append(train_averages['movie'][movie])

            # finalley, The actual Rating of this user-movie pair...
            row.append(rating)
            count = count + 1
            final_data = final_data.append([row])
            print(count)



            if (count)%10000 == 0:
                # print(','.join(map(str, row)))
                print("Done for {} rows----- {}".format(count, datetime.now() - st))

final_data.columns=['user', 'movie', 'GAvg', 'sur1', 'sur2', 'sur3', 'sur4', 'sur5', 'smr1', 'smr2', 'smr3', 'smr4', 'smr5', 'UAvg', 'MAvg', 'rating']

final_data.shape

final_data.head()

#train_pred = train_pred[:10000]

final_data['mf_svd']=predictions
final_data.head()

final_data.index

test_dataset

# Creating a sparse matrix
test_sparse_matrix = sparse.csr_matrix((test_dataset.rating.values, (test_dataset.user_id.values, test_dataset.item_id.values)))

# Global avg of all movies by all users

test_averages = dict()
# get the global average of ratings in our train set.
test_global_average = test_sparse_matrix.sum()/test_sparse_matrix.count_nonzero()
test_averages['global'] = test_global_average
test_averages

# get the user averages in dictionary (key: user_id/movie_id, value: avg rating)

def get_average_ratings(sparse_matrix, of_users):

    # average ratings of user/axes
    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes

    # ".A1" is for converting Column_Matrix to 1-D numpy array
    sum_of_ratings = sparse_matrix.sum(axis=ax).A1
    # Boolean matrix of ratings ( whether a user rated that movie or not)
    is_rated = sparse_matrix!=0
    # no of ratings that each user OR movie..
    no_of_ratings = is_rated.sum(axis=ax).A1

    # max_user  and max_movie ids in sparse matrix
    u,m = sparse_matrix.shape
    # creae a dictonary of users and their average ratigns..
    average_ratings = { i : sum_of_ratings[i]/no_of_ratings[i]
                                 for i in range(u if of_users else m)
                                    if no_of_ratings[i] !=0}

    # return that dictionary of average ratings
    return average_ratings

# Average ratings given by a user

test_averages['user'] = get_average_ratings(test_sparse_matrix, of_users=True)
#
print('\nAverage rating of user 10 :',test_averages['user'][10])

# Average ratings given for a movie

test_averages['movie'] =  get_average_ratings(test_sparse_matrix, of_users=False)
#print('\n AVerage rating of movie 15 :',test_averages['movie'][15])

#get users, movies and ratings from our samples train sparse matrix
test_users, test_movies, test_ratings = sparse.find(test_sparse_matrix)

final_test_data = pd.DataFrame()
count = 0
for (user, movie, rating)  in zip(test_users, test_movies, test_ratings):
            st = datetime.now()
        #     print(user, movie)
            #--------------------- Ratings of "movie" by similar users of "user" ---------------------
            # compute the similar Users of the "user"
            user_sim = cosine_similarity(test_sparse_matrix[user], test_sparse_matrix).ravel()
            top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.
            # get the ratings of most similar users for this movie
            top_ratings = test_sparse_matrix[top_sim_users, movie].toarray().ravel()
            # we will make it's length "5" by adding movie averages to .
            top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])
            top_sim_users_ratings.extend([test_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))
        #     print(top_sim_users_ratings, end=" ")


            #--------------------- Ratings by "user"  to similar movies of "movie" ---------------------
            # compute the similar movies of the "movie"
            movie_sim = cosine_similarity(test_sparse_matrix[:,movie].T, test_sparse_matrix.T).ravel()
            top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.
            # get the ratings of most similar movie rated by this user..
            top_ratings = test_sparse_matrix[user, top_sim_movies].toarray().ravel()
            # we will make it's length "5" by adding user averages to.
            top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])
            top_sim_movies_ratings.extend([test_averages['user'][user]]*(5-len(top_sim_movies_ratings)))
        #     print(top_sim_movies_ratings, end=" : -- ")

            #-----------------prepare the row to be stores in a file-----------------#
            row = list()
            row.append(user)
            row.append(movie)
            # Now add the other features to this data...
            row.append(test_averages['global']) # first feature
            # next 5 features are similar_users "movie" ratings
            row.extend(top_sim_users_ratings)
            # next 5 features are "user" ratings for similar_movies
            row.extend(top_sim_movies_ratings)
            # Avg_user rating
            row.append(test_averages['user'][user])
            # Avg_movie rating
            row.append(test_averages['movie'][movie])

            # finalley, The actual Rating of this user-movie pair...
            row.append(rating)
            count = count + 1
            final_test_data = final_test_data.append([row])
            print(count)



            if (count)%10000 == 0:
                # print(','.join(map(str, row)))
                print("Done for {} rows----- {}".format(count, datetime.now() - st))

final_test_data.columns = ['user', 'movie', 'GAvg', 'sur1', 'sur2', 'sur3', 'sur4', 'sur5', 'smr1', 'smr2', 'smr3', 'smr4', 'smr5',
                           'UAvg', 'MAvg', 'rating']

final_test_data.shape

final_test_data.shape

final_test_data['mf_svd'] = test_predictions

"""**Creating XGBoost**"""

# CREATING XGBoost
def get_error_metrics(y_true, y_pred):
  rmse = np.sqrt(np.mean([(y_true[i] - y_pred[i]) **2 for i in range(len(y_pred))]))
  mape = np.mean(np.abs((y_true - y_pred)/y_true)) * 100
  return rmse, mape

# prepare train data
x_train = final_data.drop(['user', 'movie','rating'], axis=1)
y_train = final_data['rating']

x_train.dtype()

# Prepare Test data
x_test = final_test_data.drop(['user','movie','rating'], axis=1)
y_test = final_test_data['rating']

import xgboost as xgb

# initialize XGBoost model...
xgb_model = xgb.XGBRegressor(silent=False, n_jobs=13, random_state=15, n_estimators=100)
# dictionaries for storing train and test results
train_results = dict()
test_results = dict()


# fit the model
print('Training the model..')
start =datetime.now()
xgb_model.fit(x_train, y_train, eval_metric = 'rmse')
print('Done. Time taken : {}\n'.format(datetime.now()-start))
print('Done \n')

# from the trained model, get the predictions....
print('Evaluating the model with TRAIN data...')
start =datetime.now()
y_train_pred = xgb_model.predict(x_train)
# get the rmse and mape of train data...
rmse_train, mape_train = get_error_metrics(y_train.values, y_train_pred)

# store the results in train_results dictionary..
train_results = {'rmse': rmse_train,
                 'mape' : mape_train,
                 'predictions' : y_train_pred}
#Evaluating the model with TRAIN data...

train_results

####################################### # get the test data predictions and compute rmse and mape
print('Evaluating Test data')
y_test_pred = xgb_model.predict(x_test)
rmse_test, mape_test = get_error_metrics(y_true=y_test.values, y_pred=y_test_pred)
# store them in our test results dictionary.
test_results = {'rmse': rmse_test,
                'mape' : mape_test,
                'predictions':y_test_pred}
#Evaluating Test data
test_results

#  Features which represent the top 5 similar users
from sklearn.metrics.pairwise import cosine_similarity

# compute the similar Users of the "user"
user_sim = cosine_similarity(train_sparse_matrix[user], train_sparse_matrix).ravel()
top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.
# get the ratings of most similar users for this movie
top_ratings = train_sparse_matrix[top_sim_users, movie].toarray().ravel()

# we will make it's length "5" by adding movie averages to
top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])
top_sim_users_ratings.extend([train_averages['movie'][movie]]*(5 -len(top_sim_users_ratings)))

# Features which represent the top 5 similar movies

# compute the similar movies of the "movie"
movie_sim = cosine_similarity(train_sparse_matrix[:,movie].T,
train_sparse_matrix.T).ravel()
top_sim_movies = movie_sim.argsort()[::-1][1:]
# we are ignoring 'The User' from its similar users.
# get the ratings of most similar movie rated by this user
top_ratings = train_sparse_matrix[user, top_sim_movies].toarray().ravel()
# we will make it's length "5" by adding user averages to
top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])
top_sim_movies_ratings.extend([train_averages['user'][user]]*(5-len(top_sim_movies_ratings)))